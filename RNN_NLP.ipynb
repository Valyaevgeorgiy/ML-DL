{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKMq7dp2W15Y",
    "outputId": "ce2273c5-6a96-4216-9d88-fbee51bf5ff0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm-QilGISxkt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Классификация фамилий (RNN)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/frNchuaBQVLxyA?w=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "YdPr92i6k-If",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.1 Используя класс `nn.RNNCell` (абстракцию для отдельного временного шага RNN), реализуйте простейшую рекуррентную сеть Элмана в виде класса `RNN`. Используя созданный класс `RNN`, решите задачу классификации фамилий. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "    tokens = set()\n",
    "    max_seq_len = 0\n",
    "    for item in data:\n",
    "        max_seq_len = max(max_seq_len, len(item))\n",
    "        tokens.update(item)\n",
    "\n",
    "    self.idx_to_token = {0: '<PAD>'}\n",
    "    self.token_to_idx = {'<PAD>': 0}\n",
    "    for idx, token in enumerate(tokens, start=1):\n",
    "        self.idx_to_token[idx] = token\n",
    "        self.token_to_idx[token] = idx\n",
    "    self.vocab_len = len(self.idx_to_token)\n",
    "    self.max_seq_len = max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, surname):\n",
    "    surname_t = torch.zeros(self.vocab.max_seq_len, dtype=torch.int64)\n",
    "    for i, token in enumerate(surname):\n",
    "        if i >= self.vocab.max_seq_len:\n",
    "            break\n",
    "        surname_t[i] = self.vocab.token_to_idx.get(token, 0)\n",
    "    return surname_t\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    surname = self.X.iloc[idx]\n",
    "    label = self.y.iloc[idx]\n",
    "    surname_t = self.vectorize(surname)\n",
    "    return surname_t, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "surnames = pd.read_csv(\"data/surnames.csv\")\n",
    "surnames['nationality'], _ = pd.factorize(surnames['nationality'])\n",
    "\n",
    "X = surnames['surname'].str.lower()\n",
    "y = surnames['nationality']\n",
    "n_classes = y.nunique()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "vocab = Vocab(X)\n",
    "\n",
    "train_dataset = SurnamesDataset(X_train, y_train, vocab)\n",
    "test_dataset = SurnamesDataset(X_test, y_test, vocab)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_model(model, train_dataloader, test_dataloader, criterion, optimizer, num_epochs):\n",
    "    model.to(device)\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, test_loss = 0, 0\n",
    "        for inputs, labels in train_dataloader:\n",
    "            x = inputs.to(device)\n",
    "            y = labels.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Валидация на val_loader\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss/len(train_dataloader))\n",
    "        test_losses.append(test_loss/len(test_dataloader))\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    #model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for surnames, labels in dataloader:\n",
    "            x = surnames#.to(device)\n",
    "            y = labels#.to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.5f}')\n",
    "\n",
    "def predict(model, dataset, surname):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vectorized = dataset.vectorize(surname)\n",
    "        tensor = vectorized.unsqueeze(0).to(device)\n",
    "\n",
    "        logits = model(tensor)\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1).squeeze()\n",
    "        top3_probs, top3_indices = torch.topk(probs, k=3)\n",
    "        print(top3_probs, top3_indices)\n",
    "\n",
    "        top3_nationalities = _[top3_indices.detach().cpu().numpy()]\n",
    "        print(f'{surname}: {top3_nationalities.values[0]} ({top3_probs[0].item():.4f}), {top3_nationalities.values[1]} ({top3_probs[1].item():.4f}), {top3_nationalities.values[2]} ({top3_probs[2].item():.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5150,  0.0611, -1.5019, -1.3999,  1.8583,  1.1652,  1.8009,\n",
      "           0.8552,  0.7180,  2.2732],\n",
      "         [ 0.9502,  1.8316, -0.1161, -1.3544,  0.6022,  0.6594, -0.2064,\n",
      "           1.6444, -0.8585,  0.1131],\n",
      "         [ 0.3644, -0.3035,  1.6394,  0.2357, -1.8279, -1.1180,  2.5095,\n",
      "          -0.3691, -1.2616, -0.3589]],\n",
      "\n",
      "        [[-0.8034,  0.3764, -0.6987,  0.3136, -1.1621,  0.0486,  0.4882,\n",
      "          -2.0484, -1.2965, -0.5811],\n",
      "         [ 1.4812, -0.3783,  1.1985,  0.2440,  1.0847,  0.6997, -1.5057,\n",
      "          -0.2568,  0.9280, -1.2979],\n",
      "         [ 0.3636, -1.2794, -1.0562, -1.7243,  0.6841,  0.3693,  1.0832,\n",
      "          -1.1520,  0.0174,  1.0618]],\n",
      "\n",
      "        [[ 0.9456, -0.2165,  1.0915,  1.2426, -0.3918, -0.0583,  1.1039,\n",
      "           0.1533, -0.5311,  0.5147],\n",
      "         [-0.0715,  0.9403, -1.9025,  0.5476,  2.0563, -0.8737, -0.2694,\n",
      "           0.9178, -0.8736,  1.6076],\n",
      "         [-0.3311, -0.0619,  0.2531, -0.3562, -0.0111,  1.1841,  0.2079,\n",
      "          -1.2520,  0.4783,  0.8634]],\n",
      "\n",
      "        [[-1.2784, -1.0371,  0.3490, -1.0331, -0.1745, -1.6130, -1.3518,\n",
      "           2.9058, -0.9466, -1.1029],\n",
      "         [ 0.3795,  0.0755,  0.1137, -0.3657,  0.1136,  0.3045, -0.4513,\n",
      "           1.3352,  0.5212,  0.9159],\n",
      "         [ 0.4075, -0.9834, -2.2925, -0.2636,  0.5778,  1.7429, -1.8118,\n",
      "           0.5036,  0.3236,  0.0416]],\n",
      "\n",
      "        [[ 1.1597, -1.0033,  0.4702,  2.3135,  0.0307, -2.1056,  0.9208,\n",
      "           0.2581, -0.2599,  0.9180],\n",
      "         [-0.8847, -0.4376, -0.8097, -1.8617,  0.1899,  3.0184,  1.0868,\n",
      "          -0.1512, -0.0273,  0.4958],\n",
      "         [ 0.3827,  0.2289, -0.7985,  1.1849, -0.1113, -0.8911, -1.1825,\n",
      "          -1.8980, -0.7876,  0.2168]],\n",
      "\n",
      "        [[ 1.4012, -0.1799, -1.3110, -0.2497,  1.7201, -0.0331,  0.2684,\n",
      "          -2.0361, -1.0382, -1.5784],\n",
      "         [-1.0465, -0.3010, -0.6830,  0.4884,  0.6238, -0.2883,  1.6776,\n",
      "           0.4518,  0.2085,  2.0163],\n",
      "         [-0.2053,  0.3691, -0.1955,  0.3405, -0.0963, -0.2021,  1.0822,\n",
      "          -1.2451,  0.3543, -1.2661]]])\n",
      "torch.Size([6, 3, 10])\n",
      "tensor([[-0.9145,  1.5498,  0.7789, -2.6064,  0.9721, -1.2638,  1.5374,  0.2785,\n",
      "          0.6504, -0.6038, -1.0444,  0.0591, -0.6605, -0.6361, -0.6395,  0.5700,\n",
      "          1.6036, -1.3799,  1.4988,  0.9585],\n",
      "        [ 1.2336, -0.5629, -0.3291,  0.6501, -2.2580,  0.1176,  0.3639,  1.4324,\n",
      "          0.3313,  0.8481,  1.5558,  0.6688, -0.2775,  0.7963, -1.7204,  0.1771,\n",
      "         -0.7053,  0.2349, -0.5183, -1.0536],\n",
      "        [-0.5980, -0.4554,  0.6471, -0.2280,  0.1770, -1.4025, -1.6942,  1.5810,\n",
      "          0.9161,  0.3777, -0.9603,  1.0469, -1.4632,  0.6301,  1.1172, -0.9917,\n",
      "          1.6730,  1.1620, -0.4439, -1.7393]]) torch.Size([3, 20])\n",
      "tensor([[-0.0441,  0.5508, -0.3919, -0.6514, -0.0477, -0.6857,  0.1197, -0.8900,\n",
      "          0.4010, -0.7310,  0.9600,  0.7814, -0.9683,  0.2209, -0.6828,  0.0841,\n",
      "          0.2634,  0.0894,  0.0892,  0.6381],\n",
      "        [-0.1870,  0.4524, -0.7628, -0.2666,  0.8309,  0.6979, -0.7682, -0.3966,\n",
      "          0.7184,  0.4711, -0.6722, -0.1217, -0.5239, -0.4409,  0.3023, -0.3200,\n",
      "          0.3488, -0.0509,  0.1939,  0.6333],\n",
      "        [-0.4637, -0.6085,  0.2977, -0.5181, -0.0406, -0.0474, -0.1065, -0.7905,\n",
      "          0.2190,  0.3209,  0.7994, -0.6154, -0.0743,  0.0762, -0.6211,  0.5210,\n",
      "         -0.1941, -0.5675, -0.0287, -0.3122]], grad_fn=<TanhBackward0>) torch.Size([3, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0441,  0.5508, -0.3919, -0.6514, -0.0477, -0.6857,  0.1197, -0.8900,\n",
       "           0.4010, -0.7310,  0.9600,  0.7814, -0.9683,  0.2209, -0.6828,  0.0841,\n",
       "           0.2634,  0.0894,  0.0892,  0.6381],\n",
       "         [-0.1870,  0.4524, -0.7628, -0.2666,  0.8309,  0.6979, -0.7682, -0.3966,\n",
       "           0.7184,  0.4711, -0.6722, -0.1217, -0.5239, -0.4409,  0.3023, -0.3200,\n",
       "           0.3488, -0.0509,  0.1939,  0.6333],\n",
       "         [-0.4637, -0.6085,  0.2977, -0.5181, -0.0406, -0.0474, -0.1065, -0.7905,\n",
       "           0.2190,  0.3209,  0.7994, -0.6154, -0.0743,  0.0762, -0.6211,  0.5210,\n",
       "          -0.1941, -0.5675, -0.0287, -0.3122]], grad_fn=<TanhBackward0>)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNNCell(10, 20)\n",
    "input = torch.randn(6, 3, 10)\n",
    "\n",
    "print(input)\n",
    "print(input.shape)\n",
    "hx = torch.randn(3, 20)\n",
    "print(hx, hx.shape)\n",
    "\n",
    "output = []\n",
    "for i in range(1):\n",
    "    hx = rnn(input[i], hx)\n",
    "    print(hx, hx.shape)\n",
    "    output.append(hx)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ir6UUkl6l4tp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
    "    super(RNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "    self.rnn_cell = nn.RNNCell(embedding_dim, hidden_size)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    x.shape = (batch_size, seq_len) - тензор входных данных\n",
    "    h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
    "    '''\n",
    "    batch_size, seq_len = x.shape\n",
    "\n",
    "    h = torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "    hidden_states = []\n",
    "    for t in range(seq_len):\n",
    "      # получаем эмбеддинг текущего символа\n",
    "      x_t = self.embedding(x[:, t])\n",
    "      # обновляем скрытое состояние\n",
    "      h = self.rnn_cell(x_t, h)\n",
    "      hidden_states.append(h)\n",
    "\n",
    "    # конкатенируем скрытые состояния и применяем полносвязный слой\n",
    "    hidden_states = torch.stack(hidden_states, dim=1)\n",
    "    output = self.fc(hidden_states[:, -1, :])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 38,  2,  6, 38, 27, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.vectorize(\"valyaev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "            input_size=vocab.vocab_len,\n",
    "            embedding_dim=256,\n",
    "            hidden_size=1024,\n",
    "            output_size=len(set(y_train)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 1345554\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Количество обучаемых параметров: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2276, -0.1097,  0.2088,  0.0419, -0.0660, -0.2510,  0.3035, -0.0100,\n",
       "          0.2670, -0.0007, -0.1839, -0.1158, -0.0880,  0.2225, -0.0344,  0.1456,\n",
       "          0.2933, -0.4632]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.vectorize(\"valyaev\").unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3323, Test Loss: 2.2180\n",
      "Epoch 2, Train Loss: 2.2998, Test Loss: 2.3518\n",
      "Epoch 3, Train Loss: 2.3117, Test Loss: 2.4430\n",
      "Epoch 4, Train Loss: 2.3174, Test Loss: 2.3229\n",
      "Epoch 5, Train Loss: 2.3151, Test Loss: 2.2831\n",
      "Epoch 6, Train Loss: 2.2317, Test Loss: 2.1473\n",
      "Epoch 7, Train Loss: 2.1554, Test Loss: 2.0496\n",
      "Epoch 8, Train Loss: 2.0836, Test Loss: 2.0341\n",
      "Epoch 9, Train Loss: 2.1080, Test Loss: 2.0658\n",
      "Epoch 10, Train Loss: 2.0116, Test Loss: 2.0219\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.40118\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a2MIErKTo9aO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.2 Замените модуль `RNN` из 1.1 на модули `nn.RNN`, `nn.LSTM` и `nn.GRU` (не забудьте указать аргумент `batch_first=True`). Сравните результаты работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
    "    super(RNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "    self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "    x.shape = (batch_size, seq_len) - тензор входных данных\n",
    "    h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
    "    '''\n",
    "    batch_size, seq_len = x.shape\n",
    "\n",
    "    # h = torch.zeros(1, batch_size, self.hidden_size)  # RNN, GRU\n",
    "    h = (torch.zeros(1, batch_size, self.hidden_size, device=device), torch.zeros(1, batch_size, self.hidden_size, device=device))  # LSTM\n",
    "\n",
    "    # получаем эмбеддинг всех символов\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # обновляем скрытое состояние\n",
    "    # output, h = self.rnn(x, h)\n",
    "    output, (h, c) = self.lstm(x, h)\n",
    "    # output, h = self.gru(x, h)\n",
    "\n",
    "    # применяем полносвязный слой\n",
    "    output = self.fc(output[:, -1, :])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "            input_size=vocab.vocab_len,\n",
    "            embedding_dim=64,\n",
    "            hidden_size=256,\n",
    "            output_size=len(set(y_train)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 667666\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Количество обучаемых параметров: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.1615, Test Loss: 1.9282\n",
      "Epoch 2, Train Loss: 1.6983, Test Loss: 1.4070\n",
      "Epoch 3, Train Loss: 1.3010, Test Loss: 1.2051\n",
      "Epoch 4, Train Loss: 1.0760, Test Loss: 1.0553\n",
      "Epoch 5, Train Loss: 0.9250, Test Loss: 0.9332\n",
      "Epoch 6, Train Loss: 0.8041, Test Loss: 0.8761\n",
      "Epoch 7, Train Loss: 0.7401, Test Loss: 0.8512\n",
      "Epoch 8, Train Loss: 0.6642, Test Loss: 0.8069\n",
      "Epoch 9, Train Loss: 0.6058, Test Loss: 0.7892\n",
      "Epoch 10, Train Loss: 0.5376, Test Loss: 0.7740\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Результаты для nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.57195\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Результаты для nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.76685\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Результаты для nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.77231\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_6YBam_3t-fO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.3 Загрузите предобученные эмбеддинги (https://disk.yandex.ru/d/BHuT2tEXr_yBOQ?w=1) в модуль `nn.Embedding` и обучите модели из 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
       "        [ 0.0134,  0.2368, -0.1690,  ..., -0.5666,  0.0447,  0.3039],\n",
       "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
       "        ...,\n",
       "        [-0.5118,  0.0587,  1.0913,  ..., -0.2500, -1.1250,  1.5863],\n",
       "        [-0.7590, -0.4743,  0.4737,  ...,  0.7895, -0.0141,  0.6448],\n",
       "        [ 0.0726, -0.5139,  0.4728,  ..., -0.1891, -0.5902,  0.5556]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_path = \"data/embeddings/glove.6B.50d.txt\"\n",
    "\n",
    "embeddings = {}\n",
    "with open(embedding_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor([float(val) for val in values[1:]])\n",
    "        embeddings[word] = vector\n",
    "\n",
    "input_size = len(embeddings)\n",
    "embedding_dim = len(next(iter(embeddings.values())))\n",
    "embedding_matrix = torch.zeros(input_size, embedding_dim)\n",
    "\n",
    "for i, word in enumerate(embeddings):\n",
    "    embedding_matrix[i] = embeddings[word]\n",
    "\n",
    "embedding_layer = nn.Embedding(input_size, embedding_dim)\n",
    "embedding_layer.weight.data.copy_(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, pretrained_embeddings):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        '''\n",
    "        x.shape = (batch_size, seq_len) - тензор входных данных\n",
    "        h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
    "        '''\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # h = torch.zeros(1, batch_size, self.hidden_size, device=device)  # RNN, GRU\n",
    "        h = (torch.zeros(1, batch_size, self.hidden_size, device=device), torch.zeros(1, batch_size, self.hidden_size, device=device))  # LSTM\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # обновляем скрытое состояние\n",
    "        # output, h = self.rnn(x, h)\n",
    "        output, (h, c) = self.lstm(x, h)\n",
    "        # output, h = self.gru(x, h)\n",
    "\n",
    "        output = self.fc(output[:, -1, :])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_size = vocab.vocab_len\n",
    "embedding_dim = len(next(iter(embeddings.values())))\n",
    "hidden_size = 128\n",
    "output_size = len(set(y_train))\n",
    "model = RNN(input_size, embedding_dim, hidden_size, output_size, pretrained_embeddings=embedding_matrix)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 186642\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Количество обучаемых параметров: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.2528, Test Loss: 2.2244\n",
      "Epoch 2, Train Loss: 2.2306, Test Loss: 2.2346\n",
      "Epoch 3, Train Loss: 2.2308, Test Loss: 2.2325\n",
      "Epoch 4, Train Loss: 2.2241, Test Loss: 2.2092\n",
      "Epoch 5, Train Loss: 2.0209, Test Loss: 1.9304\n",
      "Epoch 6, Train Loss: 1.7222, Test Loss: 1.5226\n",
      "Epoch 7, Train Loss: 1.3084, Test Loss: 1.1830\n",
      "Epoch 8, Train Loss: 1.0871, Test Loss: 1.0718\n",
      "Epoch 9, Train Loss: 0.9469, Test Loss: 1.0425\n",
      "Epoch 10, Train Loss: 0.8608, Test Loss: 0.9125\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Результаты для nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.31740\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Результаты для nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.72678\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Результаты для nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.75137\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "f7kf990U9Do-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Классификация обзоров на фильмы (RNN)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
    "\n",
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста\n",
    "\n",
    "2.2. Обучите классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
    "    - подберите адекватную размерность вектора эмбеддинга: \n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте рекуррентные слои (`nn.RNN`, `nn.LSTM`, `nn.GRU`)\n",
    "\n",
    "\n",
    "2.3 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"polarity/positive_reviews.txt\") as f:\n",
    "    positive_reviews = [elem for elem in sent_tokenize(f.read()) if elem != \".\"]\n",
    "\n",
    "with open(\"polarity/negative_reviews.txt\") as f:\n",
    "    negative_reviews = [elem for elem in sent_tokenize(f.read()) if elem != \".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6319, 6120)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_reviews), len(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it's so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[garbus] discards the potential for pathologic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12434</th>\n",
       "      <td>may prove to be [tsai's] masterpiece .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12435</th>\n",
       "      <td>mazel tov to a film about a family's joyous li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12436</th>\n",
       "      <td>standing in the shadows of motown is the best ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12437</th>\n",
       "      <td>it's nice to see piscopo again after all these...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12438</th>\n",
       "      <td>provides a porthole into that noble , tremblin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12439 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category\n",
       "0                       simplistic , silly and tedious .         1\n",
       "1      it's so laddish and juvenile , only teenage bo...         1\n",
       "2      exploitative and largely devoid of the depth o...         1\n",
       "3      [garbus] discards the potential for pathologic...         1\n",
       "4      a visually flashy but narratively opaque and e...         1\n",
       "...                                                  ...       ...\n",
       "12434             may prove to be [tsai's] masterpiece .         0\n",
       "12435  mazel tov to a film about a family's joyous li...         0\n",
       "12436  standing in the shadows of motown is the best ...         0\n",
       "12437  it's nice to see piscopo again after all these...         0\n",
       "12438  provides a porthole into that noble , tremblin...         0\n",
       "\n",
       "[12439 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.DataFrame()\n",
    "\n",
    "reviews_df[\"text\"] = positive_reviews + negative_reviews\n",
    "reviews_df[\"category\"] = [1 for i in range(len(positive_reviews))] + [0 for i in range(len(negative_reviews))]\n",
    "\n",
    "reviews_df = reviews_df\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simplistic , silly and tedious .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it s so laddish and juvenile , only teenage bo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exploitative and largely devoid of the depth o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>garbus discard the potential for pathological ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a visually flashy but narratively opaque and e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12434</th>\n",
       "      <td>may prove to be tsai s masterpiece .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12435</th>\n",
       "      <td>mazel tov to a film about a family s joyous li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12436</th>\n",
       "      <td>standing in the shadow of motown is the best k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12437</th>\n",
       "      <td>it s nice to see piscopo again after all these...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12438</th>\n",
       "      <td>provides a porthole into that noble , tremblin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12439 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  category\n",
       "0                       simplistic , silly and tedious .         1\n",
       "1      it s so laddish and juvenile , only teenage bo...         1\n",
       "2      exploitative and largely devoid of the depth o...         1\n",
       "3      garbus discard the potential for pathological ...         1\n",
       "4      a visually flashy but narratively opaque and e...         1\n",
       "...                                                  ...       ...\n",
       "12434               may prove to be tsai s masterpiece .         0\n",
       "12435  mazel tov to a film about a family s joyous li...         0\n",
       "12436  standing in the shadow of motown is the best k...         0\n",
       "12437  it s nice to see piscopo again after all these...         0\n",
       "12438  provides a porthole into that noble , tremblin...         0\n",
       "\n",
       "[12439 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([' ' if not char.isalpha() and char not in ['.', ',', '!', '?'] else char for char in text])\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "reviews_df[\"text\"] = reviews_df[\"text\"].apply(lambda x: preprocess_text(x))\n",
    "reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = reviews_df['text'].str.lower()\n",
    "y = reviews_df['category']\n",
    "\n",
    "n_classes = y.nunique()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16510, 413)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "    self.idx_to_token = {}\n",
    "    self.token_to_idx = {}\n",
    "    self.vocab_len = 0\n",
    "    self.max_seq_len = 0\n",
    "\n",
    "    for item in data:\n",
    "      self.max_seq_len = max(self.max_seq_len, len(item))\n",
    "\n",
    "    # Добавляем токен для неизвестных слов\n",
    "    self.idx_to_token = {0: '<UNK>'}\n",
    "    self.token_to_idx = {'<UNK>': 0}\n",
    "    self.vocab_len += 1\n",
    "\n",
    "    all_words = [word for sentence in data for word in word_tokenize(sentence)]\n",
    "\n",
    "    for word in all_words:\n",
    "        if word not in self.token_to_idx:\n",
    "            self.idx_to_token[self.vocab_len] = word\n",
    "            self.token_to_idx[word] = self.vocab_len\n",
    "            self.vocab_len += 1\n",
    "\n",
    "vocab = Vocab(X)\n",
    "vocab.vocab_len, vocab.max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, review):\n",
    "    '''Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)'''\n",
    "    vec = torch.zeros(self.vocab.max_seq_len, dtype=torch.int64)\n",
    "\n",
    "    for i, word in enumerate(word_tokenize(review)):\n",
    "      if i >= self.vocab.max_seq_len:\n",
    "        break\n",
    "      vec[i] = self.vocab.token_to_idx.get(word, 0)\n",
    "\n",
    "    return vec\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    vec = self.vectorize(self.X[idx])\n",
    "    label = self.y[idx]\n",
    "    return vec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = ReviewDataset(X_train, y_train, vocab)\n",
    "test_dataset = ReviewDataset(X_test, y_test, vocab)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 2999,  2398,     2,  ...,     0,     0,     0],\n",
       "         [  343, 13999,     2,  ...,     0,     0,     0],\n",
       "         [  173,    29,   428,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  340,     2,   373,  ...,     0,     0,     0],\n",
       "         [   23,   105,   425,  ...,     0,     0,     0],\n",
       "         [   61,  2615,     2,  ...,     0,     0,     0]]),\n",
       " tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 1, 0, 0])]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x.shape = (batch_size, seq_len) - тензор входных данных\n",
    "        h.shape = (batch_size, hidden_size) - тензор со скрытым состоянием RNN\n",
    "        '''\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        h = torch.zeros(2*self.num_layers, batch_size, self.hidden_size, device=device)  # RNN, GRU\n",
    "        # h = (torch.zeros(2*num_layers, batch_size, self.hidden_size, device=device), torch.zeros(2*num_layers, batch_size, self.hidden_size, device=device))  # LSTM\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output, h = self.rnn(x, h)\n",
    "        # output, (h, c) = self.lstm(x, h)\n",
    "        output, h = self.gru(x, h)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        output = self.softmax(output)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(413, 16510)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.max_seq_len, vocab.vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = RNN(\n",
    "            input_size=vocab.vocab_len,\n",
    "            embedding_dim=128,\n",
    "            hidden_size=128,\n",
    "            output_size=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество обучаемых параметров: 2642178\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Количество обучаемых параметров: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7032, Test Loss: 0.6938\n",
      "Epoch 2, Train Loss: 0.6969, Test Loss: 0.6938\n",
      "Epoch 3, Train Loss: 0.6948, Test Loss: 0.6820\n",
      "Epoch 4, Train Loss: 0.6530, Test Loss: 0.6335\n",
      "Epoch 5, Train Loss: 0.5608, Test Loss: 0.6599\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_dataloader, test_dataloader, criterion, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.68368\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rating_labels = [\"Positive\", \"Negative\"]\n",
    "\n",
    "def predict(model, dataset, review):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vectorized = dataset.vectorize(review)\n",
    "        tensor = vectorized.unsqueeze(0).to(device)\n",
    "        logits = model(tensor)\n",
    "        probs = torch.softmax(logits, dim=1).squeeze()\n",
    "        print(probs)\n",
    "        print(f'{rating_labels[probs.argmax()]} ({probs.max():.4f}), {rating_labels[probs.argmin()]} ({probs.min():.4f}) \\n{review}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7518, 0.2482])\n",
      "Positive (0.7518), Negative (0.2482) \n",
      "This restaurant is simply amazing! The food is delicious and the service is outstanding.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"This restaurant is simply amazing! The food is delicious and the service is outstanding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1056, 0.8944])\n",
      "Negative (0.8944), Positive (0.1056) \n",
      "The menu at this restaurant is very limited and the food is nothing special. I wouldn't go back.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"The menu at this restaurant is very limited and the food is nothing special. I wouldn't go back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7721, 0.2279])\n",
      "Positive (0.7721), Negative (0.2279) \n",
      "I had the best dining experience in this restaurant. The ambiance is perfect and the staff is very friendly.\n"
     ]
    }
   ],
   "source": [
    "predict(model, train_dataset, \"I had the best dining experience in this restaurant. The ambiance is perfect and the staff is very friendly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
