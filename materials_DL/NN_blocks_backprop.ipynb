{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PqC4R7SGseKa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.1 Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f4agkY9WqPwe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.dot(self.weights, inputs) + self.bias\n",
    "    # <реализовать логику нейрона>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HJRkSkHHsb7u",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(4.8400)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = 3.14\n",
    "\n",
    "Ner = Neuron(weights, bias)\n",
    "Ner.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJvnwiyty37",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fVWF3a9vtx90",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, weights, biases):\n",
    "    self.weights = weights\n",
    "    self.biases = biases\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.mv(self.weights, inputs) + self.biases\n",
    "    # <реализовать логику слоя>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Fo-JFnHPuFCS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4.8400,  0.1700, 10.3900])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])\n",
    "\n",
    "Line = Linear(weights, biases)\n",
    "Line.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtsJzcxuyGd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Z8IizmtsuhO1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3.7900,  0.9200,  9.0850],\n        [ 6.1400, -2.1000,  6.9000],\n        [ 2.0400,  0.7610,  6.7260]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "class LinearBatch:\n",
    "  def __init__(self, weights, biases):\n",
    "    self.weights = weights\n",
    "    self.biases = biases\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    return torch.mm(inputs, self.weights) + self.biases\n",
    "\n",
    "LB = LinearBatch(weights, biases)\n",
    "LB.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ2OxH4_vBLu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from torch.nn import Linear as TorchLinear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([140, 30])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = TorchLinear(20, 30)\n",
    "X = torch.randn(140, 20)\n",
    "Y = m(X)\n",
    "Y.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2128,  0.3364,  0.0752,  ...,  0.2351,  1.0376, -0.8680],\n        [-0.1714, -0.9429,  0.1837,  ..., -0.0611,  1.2930,  0.3778],\n        [ 0.3384, -0.3082,  0.5410,  ...,  0.4397, -0.2206,  0.2846],\n        ...,\n        [-0.4076, -1.3893, -0.2202,  ...,  0.6897, -0.0628, -0.7847],\n        [-0.7865,  0.6475,  0.2328,  ...,  0.8502, -0.4547, -0.1917],\n        [ 0.1910, -0.1818, -0.7219,  ..., -0.3727, -0.9511, -0.8065]],\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IOv52EdovASs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LinearNN(TorchLinear):\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    super().__init__(in_features=n_features, out_features=n_neurons)\n",
    "    self.weights = torch.randn(n_features, n_neurons)\n",
    "    self.biases = torch.randn(n_neurons)\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return torch.matmul(inputs, self.weights) + self.biases\n",
    "    # <реализовать логику слоя>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -6.4885, -11.6820,   1.9130],\n        [ -2.7882,  -9.0996,  -3.6584],\n        [ -3.2235,  -3.3459,   1.6263]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = LinearNN(n_features=4, n_neurons=3)\n",
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "m1.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -6.4885, -11.6820,   1.9130],\n        [ -2.7882,  -9.0996,  -3.6584],\n        [ -3.2235,  -3.3459,   1.6263]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1 = m1(inputs)\n",
    "Y1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPG4UqL4wajI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.1.5 Используя решение из __2.1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RjjQIQlTxJE6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -5.2885,  -3.2191,  -0.3780,  -4.9325, -11.0479,  -8.2356,  10.9490],\n        [ 17.0942,   6.2597, -11.8401,   0.4136,  18.3528,  14.3164,   3.0448],\n        [  2.7318,   8.4768,   6.4851,  -9.4748,  -3.0124,  -3.2237,  10.9897]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "L1 = LinearNN(n_features=4, n_neurons=10)\n",
    "L2 = LinearNN(n_features=10, n_neurons=7)\n",
    "\n",
    "result = L2(L1(inputs))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 7])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Создание функций активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "jZLvMRByxSTC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.7622, -0.3136,  1.9263],\n        [ 0.3659,  0.1228,  0.2715],\n        [ 0.8438, -1.2858, -0.2400],\n        [-1.4759,  0.3299, -2.6246]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ReLU:\n",
    "  def forward(self, inputs):\n",
    "    inputs[inputs < 0] = 0\n",
    "    return inputs\n",
    "\n",
    "inputs = torch.randn(4, 3)\n",
    "relu = ReLU()\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.7622, 0.0000, 1.9263],\n        [0.3659, 0.1228, 0.2715],\n        [0.8438, 0.0000, 0.0000],\n        [0.0000, 0.3299, 0.0000]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "fXNcFlqqyKHl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.7072,  0.1823, -0.7617],\n        [ 0.0643,  0.6154, -1.0999],\n        [ 0.4359, -0.3603, -0.0665],\n        [ 0.4766,  0.4565, -1.5229]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Softmax:\n",
    "  def forward(self, inputs):\n",
    "    high = torch.exp(inputs)\n",
    "    down = torch.sum(torch.exp(inputs), dim=1).view(-1, 1)\n",
    "    return high / down\n",
    "\n",
    "inputs = torch.randn(4, 3)\n",
    "softm = Softmax()\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5489, 0.3247, 0.1263],\n        [0.3282, 0.5694, 0.1024],\n        [0.4864, 0.2194, 0.2943],\n        [0.4727, 0.4633, 0.0640]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softm.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "NzMz7HDLySxK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2451, -0.0139,  1.9679],\n        [ 1.4214,  0.1181,  0.7175],\n        [-1.2775, -0.6713,  0.1963],\n        [-1.1186, -1.0180, -1.8553]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ELU:\n",
    "  def __init__(self, alpha):\n",
    "    self.alpha = alpha\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    inputs[inputs < 0] = (torch.exp(inputs[inputs < 0]) - 1) * self.alpha\n",
    "    return inputs\n",
    "\n",
    "inputs = torch.randn(4, 3)\n",
    "elu = ELU(alpha=1)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.2174, -0.0138,  1.9679],\n        [ 1.4214,  0.1181,  0.7175],\n        [-0.7213, -0.4890,  0.1963],\n        [-0.6733, -0.6387, -0.8436]])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elu.forward(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "f9-wdj5Tz-br",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "NAyuDU9F1Vuz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2, 3, 4])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-4.6159],\n        [-1.1633],\n        [-4.4178]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = LinearNN(n_features=4, n_neurons=1)\n",
    "mse = MSELoss()\n",
    "\n",
    "Y_pred = L(inputs)\n",
    "Y_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 44.119606018066406\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE: {mse.forward(Y_pred, y)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "hQl8pJsT3HcF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CategoricalCrossentropyLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return -1 * (y_true * torch.log(y_pred)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "s7Qoupfo1ZGJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 0, 0])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                        [2, 5, -1, 2], \n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 6.8174,  4.7629,  2.7025],\n        [ 2.8747, 13.3686,  4.1879],\n        [-1.9674, -2.5868, -1.7733]])"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = LinearNN(n_features=4, n_neurons=3)\n",
    "softmax = Softmax()\n",
    "cce = CategoricalCrossentropyLoss()\n",
    "\n",
    "L(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[8.7376e-01, 1.1197e-01, 1.4266e-02],\n        [2.7702e-05, 9.9987e-01, 1.0300e-04],\n        [3.6331e-01, 1.9556e-01, 4.4112e-01]])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.forward(L(inputs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE: tensor([ 0.1349, 10.4940,  1.0125])\n"
     ]
    }
   ],
   "source": [
    "print(f\"CCE: {cce.forward(softmax.forward(L(inputs)), y)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA6dbanf44_4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "ADsZxD-h4_Os",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MSELossL2:\n",
    "  def __init__(self, lambda_, layer, W=0.1):\n",
    "    # <создать атрибут объекта alpha>\n",
    "    self.lambda_ = lambda_\n",
    "    self.layer = layer\n",
    "    self.layer.W = W\n",
    "\n",
    "  def data_loss(self, y_pred, y_true):\n",
    "    # <подсчет первого слагаемого из формулы>\n",
    "    return (y_true - y_pred) ** 2\n",
    "\n",
    "  def reg_loss(self, layer):\n",
    "    # используйте атрибуты объекта layer, в которых хранятся веса слоя\n",
    "    return self.lambda_ * (layer.W ** 2)\n",
    "\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return self.data_loss(y_pred, y_true) + self.reg_loss(self.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 0, 0])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                        [2, 5, -1, 2],\n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.8129, -2.2347, -1.3714],\n        [ 2.6880,  1.3600,  1.4253],\n        [ 1.6975,  6.1108,  3.8597]])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = LinearNN(n_features=4, n_neurons=3)\n",
    "L_out = L(inputs)\n",
    "\n",
    "softmax = Softmax()\n",
    "mse2 = MSELossL2(1.5, L_out)\n",
    "\n",
    "L_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.8620, 0.0409, 0.0970],\n        [0.6460, 0.1712, 0.1828],\n        [0.0108, 0.8949, 0.0942]])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.forward(L_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 2: tensor([[0.0340, 0.0167, 0.0244],\n",
      "        [0.1403, 0.0443, 0.0484],\n",
      "        [0.9934, 0.8159, 0.0239]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE 2: {mse2.forward(softmax.forward(L_out), y)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w049ZSdR6qQi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.4 Обратное распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBtCfSME9W7Q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xmI-QJ66WAF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
    "X = # <преобразуйте массивы numpy в тензоры torch с типом torch.float32\n",
    "y = # <преобразуйте массивы numpy в тензоры torch с типом torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpPSPYSpD9Ey",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc1sXtGd_J-y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.1.1 Реализуйте класс `SquaredLoss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llFigkqd_JRU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SquaredLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return # <реализовать логику MSE>\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = # df/dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7ForfM97UQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0KqxPJU9kAN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику нейрона>\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение производной, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет значение df/dc (созданное методом backwards у объекта MSELoss)\n",
    "    self.dweights = # df/dW\n",
    "    self.dinput =  # df/wX\n",
    "    self.dbias = # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_g_FvwvmALJd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_inputs = # <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "  for x_example, y_example in zip(X, y):\n",
    "    # forward pass\n",
    "    y_pred = # <прогон через нейрон>\n",
    "    curr_loss = # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8wjk9iPMQ4x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return # <реализовать логику MSE>\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = # df/dy^\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.2.2. Модифицируйте класс `Neuron` из __2.4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_OpuAP0Jpz1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику нейрона>\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss)\n",
    "    self.dweights = # df/dW\n",
    "    self.dbias = # df/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zqwm_7eqJim1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_inputs = # <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "    y_pred = # <прогон через нейрон>\n",
    "    curr_loss = # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.3.1 Модифицируйте класс `Linear` из __2.1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zWuhaLdSB2_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику слоя>\n",
    "\n",
    "  def backward(self, dvalues):\n",
    "    self.dweights = # df/dW\n",
    "    self.dbiases = # df/db\n",
    "    self.dinputs = # df/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axUjpPz-SvS1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXoiNxkpTziV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = inputs.clip(min=0)\n",
    "    return self.output\n",
    "  \n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues.clone()\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXhspwW6T44T",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# создание компонентов сети\n",
    "# fc1 = \n",
    "# relu1 = \n",
    "# fc2 = \n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "  # <forward pass>\n",
    "  # fc1 > relu1 > fc2 > loss\n",
    "\n",
    "  data_loss = # <прогон через функцию потерь>\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(f'epoch {epoch} mean loss {data_loss}')\n",
    "    ys.append(out)\n",
    "  \n",
    "  # <backprop> \n",
    "  # loss > fc2 > relu1 > fc1\n",
    "\n",
    "  # <шаг оптимизации для fc1>\n",
    "\n",
    "  # <шаг оптимизации для fc2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKi0OfoUkwk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
    "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
    "  ax.set_xlim(-1.05, 1.5)\n",
    "  ax.set_ylim(-0.25, 1.25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}